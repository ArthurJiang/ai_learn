* Reinforcement Learning
** Classic Reinforcement Learning
[[https://jizhi.im/blog/post/intro_q_learning][Q Learning]]
** [[https://zhuanlan.zhihu.com/p/25239682][Deep Reinforcement Learning]]
```
A -> Agent action space
S -> Environment state space
R -> Reward
P -> Policy, agent based on state s choose an action a, α = π(s)
stochastic policy: Σπ(α|s) = 1
deterministic policy: π(s): S -> A
```
*** [[https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-1-A-DQN/][Deep Q Network]]
** Paper
** Course
*** [[http://rll.berkeley.edu/deeprlcourse/][cs 294]]

*** [[http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files][ucl]]
**** [[https://financestore.blob.core.windows.net/public/arthur/learning/rl/about_rl.png][About RL]] [[https://www.youtube.com/watch?v=2pWv7GOvuf0][lecture 1]] [[https://financestore.blob.core.windows.net/public/arthur/learning/rl/slides/RL_David_Sliver/intro_RL.pdf][slides]]
Big different from other machine learning paradigms:
1. No supervisor, only reward signal
2. Delayed feedback
3. Time sequential matters
4. Agent's actions affect the subsequent data it receives
***** Reward
Goal: select actions to maximise total future reward.

***** Environmentq
Agent:
Executes action \[A_{t}\]
Receives observation \[Q_{t}\]
Receives scalar reward \[R_{t}\]

Environment:
Receives action \[A_{t}\]
Emits observation \[Q_{t+1}\]
Emits scalar reward \[R_{t+1}\]

History:
The sequence of observation, actions, rewards
\[H_{t} = Q_{1},R_{1},A_{1},...,A_{t-1},Q_{t},R_{t}\]

***** State:
The information used to determine what happens next.
\[S_{t} = f(H_{t})\]
Environment state \[S^{e}_{t}\] is the environment's private representation. Environment uses to pick the next observation/reward. Which is not usually visible to the agent. Even if \[S_{t}^{e}\] is visible, it may contain irrelevant information.
Agent state \[S^{a}_{t}\] is the agent's internal representation, agent uses it to pick next action. \[S^{a}_{t} = f(H_{t})\] 
Information state(a.k.a [[https://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE][Markov state]]), \[P[S_{t+1}|S_{t}] = P[S_{t+1}|S_{1},...,S_{t}]\]. \[S_{t}^{e}\] and \[H_{t}\] are Markov.

Full observable: agent directly observes environment state \[O_{t} = S_{t}^{a} = S_{t}^{e}\], it's a Markov decision process ([[https://en.wikipedia.org/wiki/Markov_decision_process][MDP]]).

Partially observable: agent indirectly observes environment state. \[S_{t}^{a} != S_{t}^,{e}\], it's partially observable Markov decision process (POMDP).

***** RL Agent:
An RL agent may include one or more of below components:
***** Policy: agent's behaviour function.
A map form state to action.
Deterministic policy: \[a = \pi(s)\] 
Stochastic policy: \[\pi(a|s) = \mathcal{P}[A_{t} = a|S_{t} = s]\]

***** Value function: how good is each state and/or action
Prediction of the future reward, used to evaluate the goodness/badness of states.
\[v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... | S_{t} = s]\]

***** Model: agent's representation of the environment.
Prediction of the environment next state(P)/immediate reward(R).
\[\mathcal{P}_{ss^{\prime}}^{a} = \mathbb{P}[S_{t+1} = s^{\prime}|S_{t} = s, A_{t} = a]\]

\[\mathcal{R}_{s}^{a} = \mathbb{E}[R_{t+1}|S_{t} = s, A_{t} = a]\]

***** [[https://financestore.blob.core.windows.net/public/arthur/learning/rl/rl_agent_category.png][Agent Category]]
Value Based: No policy, only value function.
Policy Based: No value function, only policy.
Actor Critic: Policy and value function.
Model Free: Policy and/or value function, no model.
Model Based: Policy and/or value function, with model.

**** MDP [[https://www.youtube.com/watch?v=lfHX2hHRMVQ][lecture 2]] [[https://financestore.blob.core.windows.net/public/arthur/learning/rl/slides/RL_David_Sliver/MDP.pdf][slides]]
MDP formally describe an environment for RL.
***** State Transition Matrix
s -> current state, \[s^{\prime}\] -> successor state, \[\mathcal{P}\] -> transition matrix.
State transition probability: \[\mathcal{P}_{ss^{\prime}} = \mathbb{P}[S_{t+1} = s^{\prime} | S_{t} = s]\]
[[https://blog.csdn.net/bendanban/article/details/44221279][draw matrix]]
\[\mathcal{P} =  
\begin{matrix}
\mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\
\vdots &  & \vdots \\
\mathcal{P}_{n1} & ... & \mathcal{P}_{nn}
\end{matrix}
\] The sum of each row is 1.

****** A Markov process/chain is a memory-less random process, a tuple \[\langle \mathcal{S},\mathcal{P} \rangle\].
\[\mathcal{S}\] is a finite set of states
\[\mathcal{P}\] is state transition probability matrix.

****** A Markov reward process is a Markov chain with values, a tuple \[\langle \mathcal{S},\mathcal{P},\mathcal{R}, \gamma \rangle\].
\[\mathcal{R}\] is reward function, \[\mathcal{R}_{s} = \mathbb{E}[R_{t+1}|S_{t}=s]\]
\[\gamma\] is discount factor, \[\gamma \in [0,1]\]
\[G_{t}\] is the total discounted reward from time-step t: \[G_{t} = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty
} \gamma^{k}R_{t+k+1}\]

****** A Markov decision process is a Markov reward process with decisions, a tuple \[\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle\].
\[\mathcal{A}\] is a finite set of actions
\[\mathcal{P}_{ss^{\prime}}^{a} = \mathbb{P}[S_{t+1}=s^{\prime}|S_{t}=s, A_{t}=a ]\]
\[R_{s}^{a}=\mathbb{E}[R_{t+1}|S_{t}=s, A_{t}=a]\]

***** Value Function
The value function v(s) an MDP is the _expected_ return starting from state s.
\[v(s) = \mathbb{E}[G_{t}|S_{t}=s]\]

The state-value function \[v_{\pi}(s)\] of an MDP is _expected_ return starting form state s, following policy \[\pi\].
\[v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s]\]

The action-value function \[q_{\pi}(s,a)\] is the _expected_ return starting from state s, taking action a, following policy \[\pi\].
\[q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s,A_{t}=a]\]

***** Bellman Equation for MRP
\[R_{t+1}\] -> immediate reward
\[\gamma v(S_{t+1})\] -> discounted value of successor state

****** Value function
Value function decompose:

\[
\begin{aligned}
v(s) &= \mathbb{E}[G_{t}|S_{t}=s] \\
&= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + ... | S_{t} = s] \\
&= \mathbb{E}[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ...) | S_{t} = s] \\
&= \mathbb{E}[R_{t+1} + \gamma(G_{t+1}) | S_{t} = s] \\
&= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_{t} = s] \\
&= \mathcal{R}_{s} + \gamma \sum_{s^{\prime} \in S}\mathcal{P}_{ss^{\prime}}v(s^{\prime})
\end{aligned}
\]

State-value function decompose:
\[v_{\pi}(s)=\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s]\]
Optimal state-value function \[v_{*}(s)\] is the maximum state-value function over all policies:
\[v_{*}(s)=\max_{\pi}v_{\pi}(s)\]
_(It's the best possible performance in the MDP, when we know the optimal value function, MDP is *solved.*)_

Action-value function decompose:
\[q_{\pi}(s,a)=\mathbb{E}_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a]\]
Optimal action-value function \[q_{*}(s,a)\] is the maximum action-value function over all policies:
\[q_{*}(s,a)=\max_{\pi}q_{\pi}(s,a)\]

Derivation：
\[
\begin{aligned}
v_{\pi}(s) &=\sum_{a \in A}\pi(a|s)q_{\pi}(s,a) \\
q_{\pi}(s,a) &=\mathcal{R}_{s}^{a} + \gamma\sum_{s^{\prime} \in S} \mathcal{P}_{ss^{\prime}}^{a}v_{\pi}(s^{\prime}) \\
&=\mathcal{R}_{s}^{a} + \gamma\sum_{s^{\prime} \in S} \mathcal{P}_{ss^{\prime}}^{a}\sum_{a^{\prime} \in A}\pi(a^{\prime}|s^{\prime})q_{\pi}(s^{\prime},a^{\prime}) \\
v_{\pi}(s) &=\sum_{a \in A}\pi(a|s)(\mathcal{R}_{s}^{a}+\gamma\sum_{s^{\prime} \in S}\mathcal{P}_{ss^{\prime}}^{a}v_{\pi}(s^{\prime}))
\end{aligned}
\]

Optimal Derivation(non-linear):
\[
\begin{aligned}
v_{*}(s) &= \max_{a}q_{*}(s,a) \\
q_{*}(s,a) &= \mathcal{R}_{s}^{a} + \gamma\sum_{s^{\prime} \in S}\mathcal{P}_{ss^{\prime}}^{a}v_{*}(s^{\prime}) \\
&= \mathcal{R}_{s}^{a} + \gamma\sum_{s^{\prime} \in S}\mathcal{P}_{ss^{\prime}}^{a}\max_{a^{\prime}}q_{*}(s,a^{\prime}) \\
v_{*}(s) &= \max_{a}\mathcal{R}_{s}^{a} + \gamma\sum_{s^{\prime} \in S}\mathcal{P}_{ss^{\prime}}^{a}v_{*}(s^{\prime}) \\
\end{aligned}
\]

Bllman equation: linear equation.

\[
\left[
\begin{array}{c}
v(1) \\
\vdots \\
v(n)
\end{array}
\right] = 
\left[
\begin{array}{c}
\mathcal{R}_{1} \\
\vdots \\
\mathcal{R}_{n}
\end{array} \right]
+ \gamma \left[
\begin{matrix}
\mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\
\vdots &  & \vdots \\
\mathcal{P}_{n1} & ... & \mathcal{P}_{nn}
\end{matrix}
\right]
\left[
\begin{array}{c}
v(1) \\
\vdots \\
v(n)
\end{array}
\right]
\]

\[
\begin{aligned}
v &= \mathcal{R} + \gamma\mathcal{P}v \\
(1 - \gamma \mathcal{P})v &= \mathcal{R} \\
v &= (1 - \gamma \mathcal{P})^{-1}\mathcal{R}
\end{aligned}
\]


Bellman expectation equation:
\[
\begin{aligned}
v_{\pi} &= \mathcal{R^{\pi}} + \gamma\mathcal{P}^{\pi}v_{\pi} \\
(1 - \gamma \mathcal{P}^{\pi})v_{\pi} &= \mathcal{R}^{\pi} \\
v_{\pi} &= (1 - \gamma \mathcal{P}^{\pi})^{-1}\mathcal{R}^{\pi}
\end{aligned}
\]

Computational complexity is \[\mathcal{O}(n^{3})\] for n states. 

****** TODO Questions: why not \[\mathcal{O}(n^{2})\] in CPU?
for i:(1, n):
 v(i) = 0;
 for j:(1, n):
   v(i) += \[R_{j}\] * \[P^{\prime -1}_{ij}\]
Answer: We need get the inverse matrix, which need get determinant firstly.
So the questions change to "What's computational complexity of the determinant?"

***** Policies
\[\pi(a|s) = \mathbb{P}[A_{t}=a|S_{t}=s]\]
\[\pi \geq \pi^{\prime}  if v_{\pi}(s) \geq  v_{\pi^{\prime}}(s), \forall s\] 
Optimal policy -> \[\pi_{*} \geq \pi, \forall \pi\]

Optimal policy achieve optimal value function: 
\[v_{\pi_{*}}(s)=v_{*}(s)\]

Optimal policy achieve optimal action-value function: 
\[q_{\pi_{*}} (s,a) = q_{*}(s,a)\]

\[
\pi_{*}(a|s)=
\begin{cases}
1,  & if \ a=\mathop{\arg\max}_{a\in A} \ q_{*}(s,a) \\
0,  & otherwise
\end{cases}
\]

A policy \[\pi \] is a distribution over action given states, fully defines the _behaviour_ of an agent.
MDP policies only depend on the _current state_ not the history.
Policies are time-independent(stationary), \[A_{t} \sim \pi(.|S_{t}), \for all t > 0\]
MDP -> \[\mathcal{M}=\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle\], policy -> \[\pi\]
State sequence \[S_{1},S_{2},...\] is a Markov process \[\langle \mathcal{S},\mathcal{P}^{\pi} \rangle\]
State and reward sequence \[S_{1},R_{2},S_{2},...\] is a Markov reward process \[\langle \mathcal{S},\mathcal{P}^{\pi},\mathcal{R}^{\pi},\gamma \rangle\]
\[\mathcal{P}_{s,s_{\prime}}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{P}_{ss^{\prime}}^{a}\]
\[\mathcal{R}_{s}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}_{s}^{a}\]

**** DP [[https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=2][lecture 3]] [[https://financestore.blob.core.windows.net/public/arthur/learning/rl/slides/RL_David_Sliver/DP.pdf][slides]]
Dynamic: sequential or temporal component to the problem.
Programming: optimising a "program", i.e. a policy.
***** Iterative Policy Evaluation
Problem: evaluate a given policy \[\pi\].
Solution: iterative application of Bellman expectation backup, \[v_{1} \rightarrow v_{2} \rightarrow ... \rightarrow v_{\pi}\].
Synchronous backups: At each iteration k+1, for all states s, update \[v_{k+1}(s)\] from \[v_{k}(s^{\prime})\], \[s^{\prime}\] is a successor state of s.
\[
\begin{aligned}
v_{k+1}(s)&=\sum_{a \in A}\pi(a|s)(\mathcal{R}_{s}^{a}+\gamma\sum_{ss^{\prime}}^{a}v_{k}(s^{\prime})) \\
v^{k+1} &= \mathcal{R}^{\pi}+\gamma\mathcal{P}^{\pi}v^{k}
\end{aligned}
\]


Evaluate policy \[\pi\]: \[v_{\pi}(s)=\mathbb{E}[R_{t+1}+\gamma R_{t+2} + ...|S_{t}=s]\].
Improve by acting greedily whit respect to \[v_{\pi}\]: \[\pi^{\prime}=greedy(v_{\pi}), \ \ \pi^{{\prime}} \rightarrow \pi^{*}\]. 
[[https://financestore.blob.core.windows.net/public/arthur/learning/rl/policy_interation.png][policy interaction]]
\[
\begin{aligned}
v_{\pi}(s) &\leq q_{\pi}(s, \pi^{\prime}(s)) = \mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s] \\
&\leq \mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma q_{\pi}(S_{t+1}, \pi^{\prime}(S_{t+1}))|S_{t}=s] \\
&\leq \mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma R_{t+2} + \gamma^{2}q_{\pi}(S_{t+2},\pi^{\prime}(S_{t+2}))|S_{t}=s] \\
&\leq \mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma R_{t+2} + ...|S_{t}=s] = v_{\pi^{\prime}}(s)
\end{aligned}
\]
 
** Library
*** Gym
Gym is a toolkit for developing and comparing reinforcement learning algorithms, which provide an environment. (I don't think this is must)
Homepage: https://gym.openai.com/
Github: https://github.com/openai/gym
Paper: https://arxiv.org/pdf/1606.01540.pdf

#+BEGIN_SRC shell :results output
  git clone https://github.com/openai/gym
  cd gym; pip install -e . # pip install -e .[all] for full installation, require more dependencies, i.e. cmake, recent pip
#+END_SRC

#+BEGIN_SRC python :results output
  import gym
  env = gym.make('CartPole-v0')
  env.reset()
  for i_episode in range(20):
      observation = env.reset()
      for t in range(1000):
          env.render()
          print(observation)
          action = env.action_space.sample()
          observation, reward, done, info = env.step(action)
          if done:
              print("Episode finished after {} timesteps".format(t + 1))
              break
#+END_SRC

**** Basic
not interface for agent, three interface for Env: 
- reset(self): rest env state, return observation
- step(self, action): one tick step, return observation, reward, done, info
- render(self, mode='human', close=False): render one frame of env, default is human friendly pop up window, close for close this window
**** Environments
- Algorithmic
- Atari
- Box2d


** Glossary
*** [[Https://zh.wikipedia.org/wiki/%E8%B2%9D%E7%88%BE%E6%9B%BC%E6%96%B9%E7%A8%8B][Bellman Equation]] 贝尔曼方程
Bellman optimality equation:
\[
V^{*}(s) = \max_{a}\{R(s, a) + r\sum_{s^{\prime}}P(s^{\prime}|s, a)V^{*}(s^{\prime})\}
\]
*** [[https://zhuanlan.zhihu.com/p/24860793][Sarsa]]
*** POMDPs -> Partially observable Markov decision process with hidden states. \[\langle \mathcal{S},\mathcal{A},\mathcal{O},\mathcal{P},\mathcal{R},\mathcal{Z},\gamma \rangle\]
\[\mathcal{O}\] is a finite set of observations.
\[\mathcal{Z}\] is an observation function, \[\mathcal{Z}_{s^{\prime}o}^{a}=\mathbb{P}[O_{t+1}=o|S_{t+1}=s^{\prime},A_{t}=a]\].
\[H_{t}=A_{0},O_{1},R_{1},...,A_{t-1},O_{t},R_{t}\]
**** Belief state b(h) is probability distribution over states, conditioned on the history h:
\[b(h)=(\mathbb{P}[S_{t}=s^{1}|H_{t}=h] \ , \ ... \ , \ \mathbb{P}[S_{t}=s^{n}|H_{t}=h])\]
*** Ergodic Markov Process
Recurrent: each state is visited an infinite number of times
Aperiodic: each state is visited without any systematic period
*** TODO [[https://zh.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E5%BC%8F][Determinant]] 行列式
    [[https://zh.wikipedia.org/wiki/%E9%80%86%E7%9F%A9%E9%98%B5][Inverse matrix]] 逆矩阵
*** TODO [[https://zh.wikipedia.org/wiki/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2][Monte Carlo tree search]] 蒙特卡罗树搜索 basic idea
*** GBDT: Gradient Boosting Decision Tree
https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d
[[https://github.com/dmlc/xgboost][GXBoost]] and [[https://github.com/Microsoft/LightGBM][LightGBM]] is the variant of Gradient Boosting algorithm, which are popular not, and LightGBM is faster.

*** Q-Learning
https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-2-A-q-learning/
